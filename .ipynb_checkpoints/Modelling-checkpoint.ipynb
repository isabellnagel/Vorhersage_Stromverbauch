{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323d7ce4-8e0d-48dd-9b0c-df38beeb4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nbimporter\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bdc9fd-e037-4ac6-933b-0d573c8f54fd",
   "metadata": {},
   "source": [
    "# 1. LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b4a5d-4cf3-4030-acd0-52a0367466a4",
   "metadata": {},
   "source": [
    "## 1.1 Sequenzen erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b5b049-643a-43d4-9e18-7a3ede278135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, target, sequence_length):\n",
    "    \"\"\"\n",
    "    Erstellt Sequenzen aus den gegebenen Daten für die Verwendung in zeitlichen Vorhersagemodellen.\n",
    "\n",
    "    Parameter:\n",
    "    data (array-ähnlich): Die Eingabedaten, aus denen die Sequenzen erstellt werden.\n",
    "    target (array-ähnlich): Die Zielwerte, die den Sequenzen folgen und vorhergesagt werden sollen.\n",
    "    sequence_length (int): Die Länge jeder Sequenz, also die Anzahl der Schritte in der Sequenz.\n",
    "\n",
    "    Rückgabewert:\n",
    "    tuple: Ein Tupel bestehend aus zwei Arrays:\n",
    "        - sequences (array): Ein Array von Sequenzen mit der Länge 'sequence_length'.\n",
    "        - labels (array): Ein Array der Zielwerte, die jeweils auf eine Sequenz folgen.\n",
    "    \"\"\"\n",
    "    \n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iteriere über die Daten und erstelle Sequenzen mit der gegebenen Länge\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data[i:i + sequence_length]  # Eingabesequenz\n",
    "        label = target[i + sequence_length]  # Zielwert, der auf die Sequenz folgt\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    \n",
    "    # Rückgabe der Sequenzen und der entsprechenden Zielwerte als NumPy-Arrays\n",
    "    return np.array(sequences), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ba44d-4458-4973-abf2-410b030e6b4a",
   "metadata": {},
   "source": [
    "## 1.2 Modell aufsetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97238d43-6f3d-419a-bc83-bc575d45b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(X_train, X_test, y_train, y_test, layers, epochs, regularization, dropout, learning_rate, early_stopping):\n",
    "    \"\"\"\n",
    "    Erstellt und trainiert ein LSTM-Modell zur Vorhersage von Zeitreihendaten.\n",
    "\n",
    "    Parameter:\n",
    "    X_train (array-ähnlich): Trainingsdaten für die Eingabevariablen.\n",
    "    X_test (array-ähnlich): Testdaten für die Eingabevariablen.\n",
    "    y_train (array-ähnlich): Trainingsdaten für die Zielvariable.\n",
    "    y_test (array-ähnlich): Testdaten für die Zielvariable.\n",
    "    layers (int): Anzahl der LSTM-Einheiten in den LSTM-Schichten.\n",
    "    epochs (int): Anzahl der Epochen, die das Modell trainiert wird.\n",
    "    regularization (float): L2-Regularisierungswert zur Vermeidung von Überanpassung.\n",
    "    dropout (float): Dropout-Rate, die während des Trainings verwendet wird.\n",
    "    learning_rate (float): Lernrate für den Adam-Optimierer.\n",
    "    early_stopping (int): Geduld für Early Stopping, die bestimmt, wie viele Epochen ohne Verbesserung der Validierungsleistung toleriert werden.\n",
    "\n",
    "    Rückgabewert:\n",
    "    tuple: Ein Tupel bestehend aus:\n",
    "        - lstm_model: Das Ergebnis des Trainingsprozesses (History-Objekt).\n",
    "        - model: Das trainierte LSTM-Modell.\n",
    "    \"\"\"\n",
    "\n",
    "    # Modellinitialisierung\n",
    "    model = Sequential()\n",
    "\n",
    "    # Erste LSTM-Schicht mit Rückgabe von Sequenzen\n",
    "    model.add(LSTM(layers, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # Zweite LSTM-Schicht ohne Rückgabe von Sequenzen\n",
    "    model.add(LSTM(layers, return_sequences=False, kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # Dense-Schicht mit 25 Neuronen und ReLU-Aktivierung\n",
    "    model.add(Dense(25, activation='relu', kernel_regularizer=l2(regularization)))\n",
    "\n",
    "    # Letzte Dense-Schicht mit einer Einheit (für die Vorhersage)\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Definition des Adam-Optimierers mit angegebener Lernrate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Kompilieren des Modells mit dem Optimierer und der Verlustfunktion 'mean_squared_error'\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Early Stopping zum Beenden des Trainings, wenn sich die Validierungsleistung nicht verbessert\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping, restore_best_weights=True)\n",
    "\n",
    "    # Training des Modells mit den Trainingsdaten und Anwendung von Early Stopping\n",
    "    lstm_model = model.fit(X_train, y_train, epochs=epochs, batch_size=20, validation_data=(X_test, y_test),\n",
    "                           callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    # Rückgabe des Trainingsverlaufs und des trainierten Modells\n",
    "    return lstm_model, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09822c-3ea7-446f-ad74-8dcad7b93369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
